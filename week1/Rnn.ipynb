{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a3048c61",
            "metadata": {},
            "source": [
                "# Recurrent Neural Networks (RNNs), LSTMs & GRUs\n",
                "\n",
                "## Week 1 - Deep Learning Foundations for Generative AI\n",
                "\n",
                "In this notebook, we will explore:\n",
                "1. **Why RNNs?** - Understanding sequence modeling\n",
                "2. **Basic RNN** - Architecture and limitations\n",
                "3. **LSTM** - Long Short-Term Memory networks\n",
                "4. **GRU** - Gated Recurrent Units\n",
                "5. **Hands-on**: Build a text generation model\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fc4b49f4",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation\n",
                "\n",
                "First, let's install and import the required libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e0e6a6d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (uncomment if running on Colab)\n",
                "# !pip install tensorflow numpy matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e465f54a",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "print(f\"Keras version: {keras.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "understanding-rnns",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Why Recurrent Neural Networks?\n",
                "\n",
                "### The Problem with Traditional Neural Networks\n",
                "\n",
                "Traditional feedforward neural networks have a major limitation: **they can't handle sequential data** where the order matters.\n",
                "\n",
                "Consider these examples:\n",
                "- **Text**: \"The cat sat on the mat\" - word order matters!\n",
                "- **Speech**: Audio signals are time-series\n",
                "- **Stock prices**: Today's price depends on yesterday's\n",
                "\n",
                "### The RNN Solution\n",
                "\n",
                "RNNs introduce the concept of **memory** through a hidden state that gets updated at each time step.\n",
                "\n",
                "```\n",
                "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "     â”‚              Hidden State             â”‚\n",
                "     â”‚         (Memory of the past)          â”‚\n",
                "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "              â†‘           â†‘           â†‘\n",
                "          â”Œâ”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”€â”´â”€â”€â”€â”\n",
                "          â”‚  RNN  â”‚ â†’ â”‚  RNN  â”‚ â†’ â”‚  RNN  â”‚\n",
                "          â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜\n",
                "              â†‘           â†‘           â†‘\n",
                "            Inputâ‚      Inputâ‚‚      Inputâ‚ƒ\n",
                "           (\"The\")     (\"cat\")     (\"sat\")\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rnn-math",
            "metadata": {},
            "source": [
                "### Mathematical Formulation\n",
                "\n",
                "At each time step $t$, an RNN computes:\n",
                "\n",
                "$$h_t = \\tanh(W_{xh} \\cdot x_t + W_{hh} \\cdot h_{t-1} + b_h)$$\n",
                "$$y_t = W_{hy} \\cdot h_t + b_y$$\n",
                "\n",
                "Where:\n",
                "- $x_t$ = input at time $t$\n",
                "- $h_t$ = hidden state at time $t$ (the \"memory\")\n",
                "- $y_t$ = output at time $t$\n",
                "- $W$ = weight matrices\n",
                "- $b$ = bias vectors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "simple-rnn-demo",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's build a simple RNN from scratch to understand the concept\n",
                "\n",
                "class SimpleRNN:\n",
                "    \"\"\"A simple RNN implementation for educational purposes.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        # Initialize weights randomly\n",
                "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
                "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
                "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
                "        self.bh = np.zeros((hidden_size, 1))\n",
                "        self.by = np.zeros((output_size, 1))\n",
                "        \n",
                "    def forward(self, inputs, h_prev):\n",
                "        \"\"\"Forward pass through one time step.\"\"\"\n",
                "        # Hidden state update\n",
                "        h_next = np.tanh(self.Wxh @ inputs + self.Whh @ h_prev + self.bh)\n",
                "        # Output\n",
                "        y = self.Why @ h_next + self.by\n",
                "        return y, h_next\n",
                "\n",
                "# Demo: Process a sequence\n",
                "rnn = SimpleRNN(input_size=10, hidden_size=20, output_size=5)\n",
                "h = np.zeros((20, 1))  # Initial hidden state\n",
                "\n",
                "# Simulate 3 time steps\n",
                "for t in range(3):\n",
                "    x = np.random.randn(10, 1)  # Random input\n",
                "    y, h = rnn.forward(x, h)\n",
                "    print(f\"Time step {t+1}: Output shape = {y.shape}, Hidden state shape = {h.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vanishing-gradient",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. The Vanishing Gradient Problem\n",
                "\n",
                "### Why Basic RNNs Struggle with Long Sequences\n",
                "\n",
                "When training RNNs with backpropagation through time (BPTT), gradients can:\n",
                "- **Vanish**: Get smaller and smaller, making long-term learning impossible\n",
                "- **Explode**: Get larger and larger, causing training instability\n",
                "\n",
                "This is why simple RNNs can't learn long-term dependencies effectively.\n",
                "\n",
                "```\n",
                "Example: Predicting the blank\n",
                "\"I grew up in France... I speak fluent ___\"\n",
                "\n",
                "The word \"France\" appears many steps before the blank.\n",
                "Simple RNNs struggle to connect them!\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gradient-demo",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstration of vanishing gradients\n",
                "\n",
                "def simulate_gradient_flow(sequence_length, gradient_factor=0.9):\n",
                "    \"\"\"Simulate how gradients diminish over time.\"\"\"\n",
                "    gradients = [1.0]  # Initial gradient\n",
                "    for _ in range(sequence_length - 1):\n",
                "        gradients.append(gradients[-1] * gradient_factor)\n",
                "    return gradients\n",
                "\n",
                "# Visualize gradient flow\n",
                "seq_lengths = [10, 50, 100]\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "for seq_len in seq_lengths:\n",
                "    grads = simulate_gradient_flow(seq_len)\n",
                "    plt.plot(grads, label=f'Sequence Length = {seq_len}')\n",
                "\n",
                "plt.xlabel('Time Steps Back')\n",
                "plt.ylabel('Gradient Magnitude')\n",
                "plt.title('Vanishing Gradient Problem in RNNs')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nAfter 100 steps, gradient is only {simulate_gradient_flow(100)[-1]:.10f} of original!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "lstm-intro",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. LSTM - Long Short-Term Memory\n",
                "\n",
                "LSTMs solve the vanishing gradient problem by introducing **gates** that control information flow.\n",
                "\n",
                "### LSTM Architecture\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚                      LSTM Cell                          â”‚\n",
                "â”‚                                                         â”‚\n",
                "â”‚   â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”          â”‚\n",
                "â”‚   â”‚  Ïƒ  â”‚     â”‚  Ïƒ  â”‚     â”‚ tanhâ”‚     â”‚  Ïƒ  â”‚          â”‚\n",
                "â”‚   â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜          â”‚\n",
                "â”‚      â”‚           â”‚           â”‚           â”‚              â”‚\n",
                "â”‚   Forget       Input       Cell       Output           â”‚\n",
                "â”‚    Gate         Gate      Update       Gate            â”‚\n",
                "â”‚      â”‚           â”‚           â”‚           â”‚              â”‚\n",
                "â”‚      â–¼           â–¼           â–¼           â–¼              â”‚\n",
                "â”‚   â•”â•â•â•â•â•â•—     â•”â•â•â•â•â•â•—     â•”â•â•â•â•â•â•—                      â”‚\n",
                "â”‚   â•‘  Ã—  â•‘ +   â•‘  Ã—  â•‘  â†’  â•‘  Ã—  â•‘ â†’ hidden state      â”‚\n",
                "â”‚   â•šâ•â•â•â•â•â•     â•šâ•â•â•â•â•â•     â•šâ•â•â•â•â•â•                      â”‚\n",
                "â”‚      â†‘                                                  â”‚\n",
                "â”‚   Cell State (Long-term memory highway)                â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "### The Three Gates:\n",
                "\n",
                "1. **Forget Gate**: Decides what to forget from cell state\n",
                "2. **Input Gate**: Decides what new information to store\n",
                "3. **Output Gate**: Decides what to output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "lstm-keras",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Building an LSTM with Keras\n",
                "\n",
                "# Create a simple LSTM model\n",
                "lstm_model = keras.Sequential([\n",
                "    layers.LSTM(64, input_shape=(10, 32), return_sequences=True),  # 10 time steps, 32 features\n",
                "    layers.LSTM(32),  # Second LSTM layer\n",
                "    layers.Dense(10, activation='softmax')  # Output layer\n",
                "])\n",
                "\n",
                "lstm_model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gru-intro",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. GRU - Gated Recurrent Unit\n",
                "\n",
                "GRU is a simplified version of LSTM with fewer parameters:\n",
                "\n",
                "### Key Differences from LSTM:\n",
                "\n",
                "| Feature | LSTM | GRU |\n",
                "|---------|------|-----|\n",
                "| Gates | 3 (forget, input, output) | 2 (reset, update) |\n",
                "| Cell State | Separate | Combined with hidden state |\n",
                "| Parameters | More | Fewer |\n",
                "| Training | Slower | Faster |\n",
                "\n",
                "### When to use which?\n",
                "- **LSTM**: When you need to model very long sequences\n",
                "- **GRU**: When you want faster training with similar performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "gru-keras",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Building a GRU with Keras\n",
                "\n",
                "gru_model = keras.Sequential([\n",
                "    layers.GRU(64, input_shape=(10, 32), return_sequences=True),\n",
                "    layers.GRU(32),\n",
                "    layers.Dense(10, activation='softmax')\n",
                "])\n",
                "\n",
                "gru_model.summary()\n",
                "\n",
                "# Compare parameter counts\n",
                "print(f\"\\nğŸ“Š Parameter Comparison:\")\n",
                "print(f\"LSTM Model: {lstm_model.count_params():,} parameters\")\n",
                "print(f\"GRU Model:  {gru_model.count_params():,} parameters\")\n",
                "print(f\"GRU has {(1 - gru_model.count_params()/lstm_model.count_params())*100:.1f}% fewer parameters!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "hands-on-intro",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. ğŸ¯ Hands-On Project: Character-Level Text Generation\n",
                "\n",
                "Now let's build a complete text generation model using LSTM!\n",
                "\n",
                "**Goal**: Train a model to generate Shakespeare-like text character by character.\n",
                "\n",
                "### Steps:\n",
                "1. Load and preprocess text data\n",
                "2. Create training sequences\n",
                "3. Build the LSTM model\n",
                "4. Train the model\n",
                "5. Generate new text!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Load the Shakespeare dataset\n",
                "\n",
                "# Download Shakespeare text\n",
                "path_to_file = keras.utils.get_file(\n",
                "    'shakespeare.txt',\n",
                "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
                ")\n",
                "\n",
                "# Read the text\n",
                "text = open(path_to_file, 'r', encoding='utf-8').read()\n",
                "\n",
                "# Print some statistics\n",
                "print(f\"ğŸ“š Dataset Statistics:\")\n",
                "print(f\"Total characters: {len(text):,}\")\n",
                "print(f\"Unique characters: {len(set(text))}\")\n",
                "print(f\"\\nğŸ“– First 500 characters:\")\n",
                "print(text[:500])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Preprocess the text\n",
                "\n",
                "# Create character to index mapping\n",
                "chars = sorted(list(set(text)))\n",
                "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
                "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
                "\n",
                "vocab_size = len(chars)\n",
                "print(f\"ğŸ“ Vocabulary size: {vocab_size}\")\n",
                "print(f\"\\nğŸ“‹ Character mapping (first 20):\")\n",
                "for i, ch in enumerate(chars[:20]):\n",
                "    print(f\"  '{ch}' â†’ {i}\", end=\"  \")\n",
                "    if (i + 1) % 5 == 0:\n",
                "        print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "create-sequences",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Create training sequences\n",
                "\n",
                "# We'll use a smaller subset for faster training in this demo\n",
                "text_subset = text[:100000]  # Use first 100K characters\n",
                "\n",
                "# Sequence length (how many characters to look at)\n",
                "seq_length = 100\n",
                "\n",
                "# Create input-output pairs\n",
                "sequences = []\n",
                "next_chars = []\n",
                "\n",
                "for i in range(0, len(text_subset) - seq_length, 3):  # Step by 3 to create more diverse samples\n",
                "    sequences.append(text_subset[i:i + seq_length])\n",
                "    next_chars.append(text_subset[i + seq_length])\n",
                "\n",
                "print(f\"ğŸ”¢ Number of training sequences: {len(sequences):,}\")\n",
                "\n",
                "# Show an example\n",
                "print(f\"\\nğŸ“Œ Example sequence:\")\n",
                "print(f\"Input:  '{sequences[0]}'\")\n",
                "print(f\"Target: '{next_chars[0]}'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "one-hot-encode",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: One-hot encode the sequences\n",
                "\n",
                "print(\"â³ Creating one-hot encoded arrays...\")\n",
                "\n",
                "# Create numpy arrays\n",
                "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.float32)\n",
                "y = np.zeros((len(sequences), vocab_size), dtype=np.float32)\n",
                "\n",
                "for i, (seq, target) in enumerate(zip(sequences, next_chars)):\n",
                "    for t, char in enumerate(seq):\n",
                "        X[i, t, char_to_idx[char]] = 1\n",
                "    y[i, char_to_idx[target]] = 1\n",
                "\n",
                "print(f\"âœ… Data shapes:\")\n",
                "print(f\"   X (input):  {X.shape}  - (samples, timesteps, features)\")\n",
                "print(f\"   y (target): {y.shape}  - (samples, vocab_size)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Build the LSTM model for text generation\n",
                "\n",
                "def build_text_generator(seq_length, vocab_size):\n",
                "    model = keras.Sequential([\n",
                "        layers.LSTM(256, input_shape=(seq_length, vocab_size), return_sequences=True),\n",
                "        layers.Dropout(0.2),\n",
                "        layers.LSTM(256),\n",
                "        layers.Dropout(0.2),\n",
                "        layers.Dense(vocab_size, activation='softmax')\n",
                "    ])\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer='adam',\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    \n",
                "    return model\n",
                "\n",
                "model = build_text_generator(seq_length, vocab_size)\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 6: Train the model\n",
                "\n",
                "print(\"ğŸš€ Starting training...\")\n",
                "print(\"(This may take a few minutes. For better results, increase epochs!)\\n\")\n",
                "\n",
                "# Train for a few epochs (increase for better results)\n",
                "history = model.fit(\n",
                "    X, y,\n",
                "    batch_size=128,\n",
                "    epochs=5,  # Increase this for better results (10-20 recommended)\n",
                "    validation_split=0.1,\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot-training",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 7: Visualize training progress\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
                "\n",
                "# Plot Loss\n",
                "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
                "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Model Loss Over Time')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot Accuracy\n",
                "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
                "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].set_title('Model Accuracy Over Time')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "generate-text",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 8: Generate text!\n",
                "\n",
                "def sample_with_temperature(predictions, temperature=1.0):\n",
                "    \"\"\"\n",
                "    Sample from predictions with temperature.\n",
                "    Higher temperature = more random, lower = more deterministic.\n",
                "    \"\"\"\n",
                "    predictions = np.asarray(predictions).astype('float64')\n",
                "    predictions = np.log(predictions + 1e-10) / temperature\n",
                "    exp_preds = np.exp(predictions)\n",
                "    predictions = exp_preds / np.sum(exp_preds)\n",
                "    probas = np.random.multinomial(1, predictions, 1)\n",
                "    return np.argmax(probas)\n",
                "\n",
                "def generate_text(model, seed_text, length=200, temperature=0.5):\n",
                "    \"\"\"\n",
                "    Generate text starting from a seed text.\n",
                "    \"\"\"\n",
                "    generated = seed_text\n",
                "    \n",
                "    for _ in range(length):\n",
                "        # Prepare input\n",
                "        x_pred = np.zeros((1, seq_length, vocab_size))\n",
                "        for t, char in enumerate(generated[-seq_length:]):\n",
                "            if char in char_to_idx:\n",
                "                x_pred[0, t, char_to_idx[char]] = 1\n",
                "        \n",
                "        # Predict next character\n",
                "        predictions = model.predict(x_pred, verbose=0)[0]\n",
                "        next_idx = sample_with_temperature(predictions, temperature)\n",
                "        next_char = idx_to_char[next_idx]\n",
                "        \n",
                "        generated += next_char\n",
                "    \n",
                "    return generated\n",
                "\n",
                "# Generate text with different temperatures\n",
                "seed_text = text[:seq_length]  # Use beginning of text as seed\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"ğŸ­ GENERATED TEXT EXAMPLES\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for temp in [0.2, 0.5, 1.0]:\n",
                "    print(f\"\\nğŸ“ Temperature = {temp} (higher = more creative)\")\n",
                "    print(\"-\"*60)\n",
                "    generated = generate_text(model, seed_text, length=200, temperature=temp)\n",
                "    print(generated[seq_length:])  # Print only the generated part\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "experiment-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. ğŸ§ª Experiment Yourself!\n",
                "\n",
                "Try modifying the following and observe the results:\n",
                "\n",
                "1. **Temperature**: Change temperature values in text generation\n",
                "2. **Epochs**: Train for more epochs (10-20) for better results\n",
                "3. **LSTM units**: Try different numbers (128, 512)\n",
                "4. **Sequence length**: Try 50 or 200 instead of 100\n",
                "5. **Replace LSTM with GRU**: Compare training speed and quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "your-experiments",
            "metadata": {},
            "outputs": [],
            "source": [
                "# YOUR EXPERIMENTS HERE\n",
                "\n",
                "# Try generating with your own seed text!\n",
                "my_seed = \"To be or not to be, that is the question whether tis nobler in the mind to suffer the slings and\"\n",
                "\n",
                "# Make sure seed is exactly seq_length characters\n",
                "if len(my_seed) < seq_length:\n",
                "    my_seed = my_seed + \" \" * (seq_length - len(my_seed))\n",
                "else:\n",
                "    my_seed = my_seed[:seq_length]\n",
                "\n",
                "print(\"ğŸ­ Your custom generation:\")\n",
                "print(\"-\"*60)\n",
                "custom_generated = generate_text(model, my_seed, length=300, temperature=0.6)\n",
                "print(custom_generated)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Comparing RNN, LSTM, and GRU\n",
                "\n",
                "Let's build and compare all three architectures!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare-models",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build comparable models with RNN, LSTM, and GRU\n",
                "\n",
                "def build_rnn_model(units=64):\n",
                "    return keras.Sequential([\n",
                "        layers.SimpleRNN(units, input_shape=(seq_length, vocab_size), return_sequences=True),\n",
                "        layers.SimpleRNN(units),\n",
                "        layers.Dense(vocab_size, activation='softmax')\n",
                "    ])\n",
                "\n",
                "def build_lstm_model(units=64):\n",
                "    return keras.Sequential([\n",
                "        layers.LSTM(units, input_shape=(seq_length, vocab_size), return_sequences=True),\n",
                "        layers.LSTM(units),\n",
                "        layers.Dense(vocab_size, activation='softmax')\n",
                "    ])\n",
                "\n",
                "def build_gru_model(units=64):\n",
                "    return keras.Sequential([\n",
                "        layers.GRU(units, input_shape=(seq_length, vocab_size), return_sequences=True),\n",
                "        layers.GRU(units),\n",
                "        layers.Dense(vocab_size, activation='softmax')\n",
                "    ])\n",
                "\n",
                "# Create models\n",
                "rnn_model = build_rnn_model()\n",
                "lstm_model = build_lstm_model()\n",
                "gru_model = build_gru_model()\n",
                "\n",
                "# Compare parameter counts\n",
                "print(\"ğŸ“Š Model Comparison:\")\n",
                "print(\"=\"*50)\n",
                "print(f\"{'Model':<15} {'Parameters':>15} {'Relative Size':>15}\")\n",
                "print(\"-\"*50)\n",
                "\n",
                "models = [\n",
                "    ('Simple RNN', rnn_model),\n",
                "    ('LSTM', lstm_model),\n",
                "    ('GRU', gru_model)\n",
                "]\n",
                "\n",
                "base_params = rnn_model.count_params()\n",
                "for name, m in models:\n",
                "    params = m.count_params()\n",
                "    relative = params / base_params\n",
                "    print(f\"{name:<15} {params:>15,} {relative:>14.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Summary & Key Takeaways\n",
                "\n",
                "### What We Learned:\n",
                "\n",
                "| Concept | Description |\n",
                "|---------|-------------|\n",
                "| **RNNs** | Neural networks with memory for sequence processing |\n",
                "| **Vanishing Gradient** | Why simple RNNs fail on long sequences |\n",
                "| **LSTM** | Solves vanishing gradient with gates (forget, input, output) |\n",
                "| **GRU** | Simplified LSTM with fewer parameters |\n",
                "| **Text Generation** | Character-by-character prediction using RNNs |\n",
                "| **Temperature** | Controls randomness in text generation |\n",
                "\n",
                "### When to Use What:\n",
                "\n",
                "- **Simple RNN**: Short sequences, simple patterns\n",
                "- **LSTM**: Long sequences, complex dependencies\n",
                "- **GRU**: Similar tasks as LSTM but faster training needed\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. Try training on different datasets (song lyrics, code, etc.)\n",
                "2. Explore Bidirectional LSTMs\n",
                "3. Learn about Attention mechanisms (Week 3)\n",
                "4. Study Transformers - the next evolution! (Week 3-4)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exercises",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. ğŸ“ Practice Exercises\n",
                "\n",
                "### Exercise 1: Temperature Exploration\n",
                "Generate text with temperatures: 0.1, 0.3, 0.7, 1.5. What patterns do you observe?\n",
                "\n",
                "### Exercise 2: GRU vs LSTM\n",
                "Replace the LSTM model with GRU. Compare:\n",
                "- Training time\n",
                "- Final loss\n",
                "- Output quality\n",
                "\n",
                "### Exercise 3: Bidirectional LSTM\n",
                "Modify the model to use `layers.Bidirectional(layers.LSTM(...))`. How does it affect performance?\n",
                "\n",
                "### Exercise 4: Custom Dataset\n",
                "Train the model on a different text dataset (your favorite book, song lyrics, etc.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exercise-space",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Space for your exercise solutions!\n",
                "\n",
                "# Exercise 1: Temperature exploration\n",
                "# TODO: Generate text with different temperatures and compare\n",
                "\n",
                "\n",
                "# Exercise 2: GRU comparison\n",
                "# TODO: Build and train a GRU model\n",
                "\n",
                "\n",
                "# Exercise 3: Bidirectional LSTM\n",
                "# TODO: Create a bidirectional model\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "resources",
            "metadata": {},
            "source": [
                "---\n",
                "## 11. ğŸ“š Additional Resources\n",
                "\n",
                "### Reading:\n",
                "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - Chris Olah's classic blog post\n",
                "- [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - Andrej Karpathy\n",
                "\n",
                "### Video:\n",
                "- [RNN & LSTM - Stanford CS231n](https://www.youtube.com/watch?v=6niqTuYFZLQ)\n",
                "\n",
                "### Code:\n",
                "- [TensorFlow RNN Tutorial](https://www.tensorflow.org/text/tutorials/text_generation)\n",
                "- [PyTorch Sequence Models](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
                "\n",
                "---\n",
                "\n",
                "**ğŸ‰ Congratulations on completing the RNN notebook!**\n",
                "\n",
                "*Next week: Variational Autoencoders (VAEs)*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}