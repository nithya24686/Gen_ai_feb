{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e0cb4b90",
            "metadata": {},
            "source": [
                "# Generative Adversarial Networks (GANs)\n",
                "\n",
                "## Week 3, Day 1 - Deep Dive into GANs\n",
                "\n",
                "### What You'll Learn:\n",
                "1. **GAN Fundamentals** - The revolutionary idea behind GANs\n",
                "2. **Generator & Discriminator** - The two-player game\n",
                "3. **Training Dynamics** - How GANs learn\n",
                "4. **DCGAN Architecture** - Deep Convolutional GANs\n",
                "5. **üéØ Live Project**: Fashion Image Generation with DCGAN\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "intro-gan",
            "metadata": {},
            "source": [
                "## 1. What are GANs?\n",
                "\n",
                "**Generative Adversarial Networks (GANs)** were introduced by Ian Goodfellow in 2014 and are considered one of the most exciting ideas in AI.\n",
                "\n",
                "> *\"The most interesting idea in the last 10 years in ML\"* - Yann LeCun, 2016\n",
                "\n",
                "### The Core Idea: A Two-Player Game\n",
                "\n",
                "Imagine a game between two players:\n",
                "\n",
                "üé® **Generator (G)**: A forger trying to create fake art\n",
                "üîç **Discriminator (D)**: An art detective trying to catch fakes\n",
                "\n",
                "```\n",
                "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "                    ‚îÇ          THE GAN GAME               ‚îÇ\n",
                "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "                                    \n",
                "    Random Noise              Generated Image              Real/Fake?\n",
                "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "    ‚îÇ z ~ N(0,1)‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ  GENERATOR  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂   ‚îÇ         ‚îÇ\n",
                "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ     (G)     ‚îÇ              ‚îÇ         ‚îÇ\n",
                "                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ DISCRIM ‚îÇ ‚îÄ‚îÄ‚ñ∂ 0 or 1\n",
                "                                                          ‚îÇ  (D)    ‚îÇ\n",
                "    Real Images                                           ‚îÇ         ‚îÇ\n",
                "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂   ‚îÇ         ‚îÇ\n",
                "    ‚îÇ Dataset ‚îÇ                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "### The Adversarial Process:\n",
                "\n",
                "1. **Generator** creates fake images from random noise\n",
                "2. **Discriminator** tries to distinguish real from fake\n",
                "3. Both improve through competition\n",
                "4. Eventually, Generator creates images so realistic that Discriminator can't tell!"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "math-section",
            "metadata": {},
            "source": [
                "## 2. Mathematical Foundation\n",
                "\n",
                "### The Minimax Game\n",
                "\n",
                "GANs optimize this objective function:\n",
                "\n",
                "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
                "\n",
                "**Breaking it down:**\n",
                "\n",
                "| Term | Meaning |\n",
                "|------|----------|\n",
                "| $D(x)$ | Discriminator's probability that x is real |\n",
                "| $G(z)$ | Generator's output from noise z |\n",
                "| $\\log D(x)$ | D wants this HIGH (correctly identify real) |\n",
                "| $\\log(1 - D(G(z)))$ | D wants this HIGH (correctly identify fakes) |\n",
                "| | G wants this LOW (fool the discriminator) |\n",
                "\n",
                "### Training Objectives:\n",
                "\n",
                "**Discriminator's Goal**: Maximize ability to distinguish real from fake\n",
                "```\n",
                "D wants: D(real) ‚Üí 1 and D(G(z)) ‚Üí 0\n",
                "```\n",
                "\n",
                "**Generator's Goal**: Minimize discriminator's ability to detect fakes\n",
                "```\n",
                "G wants: D(G(z)) ‚Üí 1 (fool D into thinking fakes are real)\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Setup & Installation\n",
                "\n",
                "Let's set up our environment for building GANs!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (uncomment for Colab)\n",
                "# !pip install tensorflow matplotlib imageio tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports-2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from IPython import display\n",
                "import time\n",
                "import os\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "simple-gan-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Building a Simple GAN from Scratch\n",
                "\n",
                "Let's first build a basic GAN to understand the mechanics before moving to DCGAN."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "simple-generator",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple Generator Network\n",
                "def build_simple_generator(latent_dim):\n",
                "    \"\"\"Generator: Maps random noise to image space.\"\"\"\n",
                "    model = keras.Sequential([\n",
                "        layers.Dense(256, input_dim=latent_dim),\n",
                "        layers.LeakyReLU(0.2),\n",
                "        layers.BatchNormalization(),\n",
                "        \n",
                "        layers.Dense(512),\n",
                "        layers.LeakyReLU(0.2),\n",
                "        layers.BatchNormalization(),\n",
                "        \n",
                "        layers.Dense(1024),\n",
                "        layers.LeakyReLU(0.2),\n",
                "        layers.BatchNormalization(),\n",
                "        \n",
                "        layers.Dense(28 * 28 * 1, activation='tanh'),\n",
                "        layers.Reshape((28, 28, 1))\n",
                "    ], name='generator')\n",
                "    return model\n",
                "\n",
                "# Simple Discriminator Network\n",
                "def build_simple_discriminator(img_shape):\n",
                "    \"\"\"Discriminator: Classifies images as real or fake.\"\"\"\n",
                "    model = keras.Sequential([\n",
                "        layers.Flatten(input_shape=img_shape),\n",
                "        \n",
                "        layers.Dense(512),\n",
                "        layers.LeakyReLU(0.2),\n",
                "        layers.Dropout(0.3),\n",
                "        \n",
                "        layers.Dense(256),\n",
                "        layers.LeakyReLU(0.2),\n",
                "        layers.Dropout(0.3),\n",
                "        \n",
                "        layers.Dense(1, activation='sigmoid')\n",
                "    ], name='discriminator')\n",
                "    return model\n",
                "\n",
                "# Test the architectures\n",
                "latent_dim = 100\n",
                "img_shape = (28, 28, 1)\n",
                "\n",
                "gen = build_simple_generator(latent_dim)\n",
                "disc = build_simple_discriminator(img_shape)\n",
                "\n",
                "print(\"Generator Summary:\")\n",
                "gen.summary()\n",
                "print(\"\\nDiscriminator Summary:\")\n",
                "disc.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dcgan-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. DCGAN - Deep Convolutional GAN\n",
                "\n",
                "DCGAN (2015) introduced architectural guidelines that made training stable:\n",
                "\n",
                "### DCGAN Guidelines:\n",
                "\n",
                "| Guideline | Generator | Discriminator |\n",
                "|-----------|-----------|---------------|\n",
                "| Convolutions | Use Transposed Conv | Use Strided Conv |\n",
                "| Batch Norm | Yes (except output) | Yes (except input) |\n",
                "| Activation | ReLU | LeakyReLU |\n",
                "| Output Activation | Tanh | Sigmoid |\n",
                "| Pooling | No Max Pooling | No Max Pooling |\n",
                "\n",
                "```\n",
                "DCGAN Generator Architecture:\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ  Noise z  ‚îÇ ‚îÄ‚ñ∂ ‚îÇ Dense +   ‚îÇ ‚îÄ‚ñ∂ ‚îÇ ConvT 256 ‚îÇ ‚îÄ‚ñ∂ ‚îÇ ConvT 128 ‚îÇ ‚îÄ‚ñ∂ ‚îÇ ConvT 64  ‚îÇ ‚îÄ‚ñ∂ Image\n",
                "‚îÇ  (100,)   ‚îÇ    ‚îÇ Reshape   ‚îÇ    ‚îÇ   7√ó7     ‚îÇ    ‚îÇ   14√ó14   ‚îÇ    ‚îÇ   28√ó28   ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcgan-models",
            "metadata": {},
            "outputs": [],
            "source": [
                "# DCGAN Generator\n",
                "def build_dcgan_generator(latent_dim=100):\n",
                "    model = keras.Sequential(name='dcgan_generator')\n",
                "    \n",
                "    # Foundation for 7x7 image\n",
                "    model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(latent_dim,)))\n",
                "    model.add(layers.BatchNormalization())\n",
                "    model.add(layers.LeakyReLU())\n",
                "    model.add(layers.Reshape((7, 7, 256)))\n",
                "    \n",
                "    # Upsample to 7x7x128\n",
                "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
                "    model.add(layers.BatchNormalization())\n",
                "    model.add(layers.LeakyReLU())\n",
                "    \n",
                "    # Upsample to 14x14x64\n",
                "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
                "    model.add(layers.BatchNormalization())\n",
                "    model.add(layers.LeakyReLU())\n",
                "    \n",
                "    # Upsample to 28x28x1\n",
                "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
                "    \n",
                "    return model\n",
                "\n",
                "# DCGAN Discriminator\n",
                "def build_dcgan_discriminator():\n",
                "    model = keras.Sequential(name='dcgan_discriminator')\n",
                "    \n",
                "    # 28x28x1 -> 14x14x64\n",
                "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
                "    model.add(layers.LeakyReLU())\n",
                "    model.add(layers.Dropout(0.3))\n",
                "    \n",
                "    # 14x14x64 -> 7x7x128\n",
                "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
                "    model.add(layers.LeakyReLU())\n",
                "    model.add(layers.Dropout(0.3))\n",
                "    \n",
                "    # Flatten and classify\n",
                "    model.add(layers.Flatten())\n",
                "    model.add(layers.Dense(1))\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Build models\n",
                "generator = build_dcgan_generator()\n",
                "discriminator = build_dcgan_discriminator()\n",
                "\n",
                "print(\"DCGAN Generator:\")\n",
                "generator.summary()\n",
                "print(\"\\nDCGAN Discriminator:\")\n",
                "discriminator.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test-generator",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the untrained generator\n",
                "noise = tf.random.normal([1, 100])\n",
                "generated_image = generator(noise, training=False)\n",
                "\n",
                "plt.figure(figsize=(4, 4))\n",
                "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
                "plt.title('Untrained Generator Output (Random Noise)')\n",
                "plt.axis('off')\n",
                "plt.show()\n",
                "\n",
                "# Test discriminator\n",
                "decision = discriminator(generated_image)\n",
                "print(f\"Discriminator output: {decision.numpy()[0][0]:.4f}\")\n",
                "print(\"(Before training, this is random)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "project-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. üéØ Live Project: Fashion Image Generation\n",
                "\n",
                "We'll train a DCGAN to generate realistic fashion items using the **Fashion-MNIST** dataset!\n",
                "\n",
                "### Why Fashion-MNIST?\n",
                "- **Real-world relevance**: Fashion industry uses GANs for design\n",
                "- **Challenging**: More complex than digits\n",
                "- **Fast training**: 28x28 images train quickly\n",
                "- **Clear quality metrics**: Easy to evaluate output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Fashion-MNIST dataset\n",
                "(train_images, train_labels), (_, _) = keras.datasets.fashion_mnist.load_data()\n",
                "\n",
                "# Fashion-MNIST class labels\n",
                "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
                "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
                "\n",
                "print(f\"Dataset shape: {train_images.shape}\")\n",
                "print(f\"Number of training images: {len(train_images):,}\")\n",
                "\n",
                "# Visualize some samples\n",
                "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    ax.imshow(train_images[i], cmap='gray')\n",
                "    ax.set_title(class_names[train_labels[i]])\n",
                "    ax.axis('off')\n",
                "plt.suptitle('Fashion-MNIST Samples', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocess-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess the data\n",
                "\n",
                "# Normalize to [-1, 1] (for tanh activation)\n",
                "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
                "train_images = (train_images - 127.5) / 127.5\n",
                "\n",
                "print(f\"Min value: {train_images.min()}, Max value: {train_images.max()}\")\n",
                "\n",
                "# Create tf.data.Dataset for efficient batching\n",
                "BUFFER_SIZE = 60000\n",
                "BATCH_SIZE = 256\n",
                "\n",
                "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
                "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
                "\n",
                "print(f\"Number of batches: {len(train_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "loss-optimizers",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define loss function and optimizers\n",
                "\n",
                "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
                "\n",
                "def discriminator_loss(real_output, fake_output):\n",
                "    \"\"\"Discriminator wants to correctly classify real and fake.\"\"\"\n",
                "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)  # Real = 1\n",
                "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)  # Fake = 0\n",
                "    total_loss = real_loss + fake_loss\n",
                "    return total_loss\n",
                "\n",
                "def generator_loss(fake_output):\n",
                "    \"\"\"Generator wants discriminator to think fakes are real.\"\"\"\n",
                "    return cross_entropy(tf.ones_like(fake_output), fake_output)  # Wants D(G(z)) = 1\n",
                "\n",
                "# Optimizers with recommended learning rates\n",
                "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
                "discriminator_optimizer = keras.optimizers.Adam(1e-4)\n",
                "\n",
                "print(\"‚úÖ Loss functions and optimizers defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training-step",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "LATENT_DIM = 100\n",
                "EPOCHS = 50  # Increase for better results\n",
                "num_examples_to_generate = 16\n",
                "\n",
                "# Seed for consistent visualization\n",
                "seed = tf.random.normal([num_examples_to_generate, LATENT_DIM])\n",
                "\n",
                "@tf.function\n",
                "def train_step(images):\n",
                "    \"\"\"Single training step for both G and D.\"\"\"\n",
                "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
                "    \n",
                "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
                "        generated_images = generator(noise, training=True)\n",
                "        \n",
                "        real_output = discriminator(images, training=True)\n",
                "        fake_output = discriminator(generated_images, training=True)\n",
                "        \n",
                "        gen_loss = generator_loss(fake_output)\n",
                "        disc_loss = discriminator_loss(real_output, fake_output)\n",
                "    \n",
                "    # Compute and apply gradients\n",
                "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
                "    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
                "    \n",
                "    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
                "    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
                "    \n",
                "    return gen_loss, disc_loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "visualization-helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_and_save_images(model, epoch, test_input):\n",
                "    \"\"\"Generate and display images during training.\"\"\"\n",
                "    predictions = model(test_input, training=False)\n",
                "    \n",
                "    fig = plt.figure(figsize=(4, 4))\n",
                "    for i in range(predictions.shape[0]):\n",
                "        plt.subplot(4, 4, i+1)\n",
                "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
                "        plt.axis('off')\n",
                "    \n",
                "    plt.suptitle(f'Epoch {epoch}', fontsize=12)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Test visualization function\n",
                "print(\"Initial random output:\")\n",
                "generate_and_save_images(generator, 0, seed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training-loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "def train(dataset, epochs):\n",
                "    gen_losses = []\n",
                "    disc_losses = []\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        start = time.time()\n",
                "        \n",
                "        epoch_gen_loss = []\n",
                "        epoch_disc_loss = []\n",
                "        \n",
                "        for image_batch in dataset:\n",
                "            g_loss, d_loss = train_step(image_batch)\n",
                "            epoch_gen_loss.append(g_loss)\n",
                "            epoch_disc_loss.append(d_loss)\n",
                "        \n",
                "        # Track average losses\n",
                "        gen_losses.append(np.mean(epoch_gen_loss))\n",
                "        disc_losses.append(np.mean(epoch_disc_loss))\n",
                "        \n",
                "        # Display progress every 10 epochs\n",
                "        if (epoch + 1) % 10 == 0:\n",
                "            display.clear_output(wait=True)\n",
                "            generate_and_save_images(generator, epoch + 1, seed)\n",
                "            print(f'Epoch {epoch+1}/{epochs}')\n",
                "            print(f'  Generator Loss: {gen_losses[-1]:.4f}')\n",
                "            print(f'  Discriminator Loss: {disc_losses[-1]:.4f}')\n",
                "            print(f'  Time: {time.time()-start:.2f}s')\n",
                "    \n",
                "    # Final generation\n",
                "    display.clear_output(wait=True)\n",
                "    generate_and_save_images(generator, epochs, seed)\n",
                "    \n",
                "    return gen_losses, disc_losses\n",
                "\n",
                "print(\"üöÄ Starting training...\")\n",
                "print(f\"Training for {EPOCHS} epochs with batch size {BATCH_SIZE}\")\n",
                "print(\"This may take 10-20 minutes depending on your hardware.\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-training",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run training!\n",
                "gen_losses, disc_losses = train(train_dataset, EPOCHS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot-losses",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training progress\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(gen_losses, label='Generator', linewidth=2)\n",
                "plt.plot(disc_losses, label='Discriminator', linewidth=2)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('GAN Training Losses')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(gen_losses, label='Generator', linewidth=2, alpha=0.7)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Generator Loss')\n",
                "plt.title('Generator Loss Over Time')\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "generate-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Generate New Fashion Items!\n",
                "\n",
                "Now let's use our trained generator to create new fashion designs!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "generate-samples",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate a grid of fashion items\n",
                "def generate_fashion_grid(generator, n_rows=4, n_cols=8):\n",
                "    \"\"\"Generate a grid of fashion items.\"\"\"\n",
                "    n_samples = n_rows * n_cols\n",
                "    noise = tf.random.normal([n_samples, LATENT_DIM])\n",
                "    generated = generator(noise, training=False)\n",
                "    \n",
                "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 8))\n",
                "    for i, ax in enumerate(axes.flat):\n",
                "        img = generated[i, :, :, 0] * 127.5 + 127.5\n",
                "        ax.imshow(img, cmap='gray')\n",
                "        ax.axis('off')\n",
                "    \n",
                "    plt.suptitle('üé® AI-Generated Fashion Items', fontsize=16)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "generate_fashion_grid(generator)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "latent-interpolation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Latent Space Interpolation - Watch fashion morph!\n",
                "def interpolate_latent_space(generator, n_steps=10):\n",
                "    \"\"\"Interpolate between two random points in latent space.\"\"\"\n",
                "    # Two random latent vectors\n",
                "    z1 = tf.random.normal([1, LATENT_DIM])\n",
                "    z2 = tf.random.normal([1, LATENT_DIM])\n",
                "    \n",
                "    # Linear interpolation\n",
                "    ratios = np.linspace(0, 1, n_steps)\n",
                "    \n",
                "    fig, axes = plt.subplots(1, n_steps, figsize=(20, 3))\n",
                "    \n",
                "    for i, ratio in enumerate(ratios):\n",
                "        z = z1 * (1 - ratio) + z2 * ratio\n",
                "        img = generator(z, training=False)\n",
                "        axes[i].imshow(img[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
                "        axes[i].axis('off')\n",
                "        axes[i].set_title(f'{ratio:.1f}')\n",
                "    \n",
                "    plt.suptitle('Latent Space Interpolation: Morphing Fashion', fontsize=14)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "print(\"Watch one fashion item morph into another!\")\n",
                "interpolate_latent_space(generator)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Real vs Generated Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare-real-fake",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare real and generated images side by side\n",
                "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
                "\n",
                "# Real images (top row)\n",
                "for i in range(8):\n",
                "    axes[0, i].imshow(train_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
                "    axes[0, i].axis('off')\n",
                "    if i == 0:\n",
                "        axes[0, i].set_title('Real Images', fontsize=12, loc='left')\n",
                "\n",
                "# Generated images (bottom row)\n",
                "noise = tf.random.normal([8, LATENT_DIM])\n",
                "generated = generator(noise, training=False)\n",
                "for i in range(8):\n",
                "    axes[1, i].imshow(generated[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
                "    axes[1, i].axis('off')\n",
                "    if i == 0:\n",
                "        axes[1, i].set_title('Generated Images', fontsize=12, loc='left')\n",
                "\n",
                "plt.suptitle('Real vs AI-Generated Fashion', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gan-types",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Types of GANs\n",
                "\n",
                "| GAN Type | Purpose | Example Use Case |\n",
                "|----------|---------|------------------|\n",
                "| **DCGAN** | Image generation with CNNs | Fashion, faces |\n",
                "| **Conditional GAN** | Generate specific classes | Generate specific fashion items |\n",
                "| **CycleGAN** | Unpaired image translation | Horse ‚Üî Zebra, Photo ‚Üî Painting |\n",
                "| **StyleGAN** | High-quality face generation | ThisPersonDoesNotExist.com |\n",
                "| **Pix2Pix** | Paired image translation | Sketch ‚Üí Photo |\n",
                "| **SRGAN** | Super-resolution | Enhance low-res images |\n",
                "\n",
                "### Famous GAN Applications:\n",
                "\n",
                "1. **DeepFakes** - Face swapping in videos\n",
                "2. **DALL-E/Midjourney** - Text to image (uses GAN concepts)\n",
                "3. **NVIDIA GauGAN** - Landscape painting tool\n",
                "4. **AI Art Generation** - Creating artwork"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "challenges",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. GAN Training Challenges & Solutions\n",
                "\n",
                "### Common Problems:\n",
                "\n",
                "| Problem | Description | Solution |\n",
                "|---------|-------------|----------|\n",
                "| **Mode Collapse** | Generator produces limited variety | Use minibatch discrimination, unrolled GANs |\n",
                "| **Training Instability** | Loss oscillates wildly | Use WGAN, spectral normalization |\n",
                "| **Vanishing Gradients** | D becomes too strong | Use feature matching, label smoothing |\n",
                "| **Non-convergence** | Training never stabilizes | Use progressive growing, careful hyperparams |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training-tips",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstration: Label Smoothing for stable training\n",
                "def discriminator_loss_with_smoothing(real_output, fake_output, smoothing=0.1):\n",
                "    \"\"\"Use label smoothing: real=0.9 instead of 1.0\"\"\"\n",
                "    real_labels = tf.ones_like(real_output) * (1 - smoothing)  # 0.9 instead of 1\n",
                "    fake_labels = tf.zeros_like(fake_output)\n",
                "    \n",
                "    real_loss = cross_entropy(real_labels, real_output)\n",
                "    fake_loss = cross_entropy(fake_labels, fake_output)\n",
                "    return real_loss + fake_loss\n",
                "\n",
                "print(\"üí° Label smoothing prevents discriminator from becoming overconfident!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 11. Summary & Key Takeaways\n",
                "\n",
                "### What We Learned:\n",
                "\n",
                "| Concept | Description |\n",
                "|---------|-------------|\n",
                "| **GANs** | Two networks in adversarial training |\n",
                "| **Generator** | Creates fake data from random noise |\n",
                "| **Discriminator** | Distinguishes real from fake |\n",
                "| **DCGAN** | Convolutional GANs for images |\n",
                "| **Latent Space** | Compressed representation of data |\n",
                "| **Mode Collapse** | When G produces limited variety |\n",
                "\n",
                "### Training Tips:\n",
                "1. Use **BatchNorm** in Generator\n",
                "2. Use **LeakyReLU** in Discriminator\n",
                "3. Apply **label smoothing**\n",
                "4. Balance G and D training\n",
                "5. Monitor both losses\n",
                "\n",
                "### Next Steps:\n",
                "- Try **Conditional GANs** for class-specific generation\n",
                "- Explore **StyleGAN** for high-quality faces\n",
                "- Learn about **CycleGAN** for image translation"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "exercises-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 12. üìù Practice Exercises\n",
                "\n",
                "### Exercise 1: Train on MNIST Digits\n",
                "Modify the code to train on MNIST digits instead of Fashion-MNIST.\n",
                "\n",
                "### Exercise 2: Experiment with Architecture\n",
                "Try adding more layers or changing the number of filters. How does it affect quality?\n",
                "\n",
                "### Exercise 3: Conditional Generation (Advanced)\n",
                "Modify the GAN to generate specific fashion categories by conditioning on class labels.\n",
                "\n",
                "### Exercise 4: Training Duration\n",
                "Train for 100+ epochs. Document the quality improvement over time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "exercise-space",
            "metadata": {},
            "outputs": [],
            "source": [
                "# YOUR EXERCISE SOLUTIONS HERE\n",
                "\n",
                "# Exercise 1: MNIST Training\n",
                "# Hint: Just change keras.datasets.fashion_mnist to keras.datasets.mnist\n",
                "\n",
                "# Exercise 2: Architecture changes\n",
                "# Hint: Try adding layers.Conv2DTranspose(256, ...) layer\n",
                "\n",
                "# Exercise 3: Conditional GAN\n",
                "# Hint: Concatenate class labels with noise input to generator\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "resources-section",
            "metadata": {},
            "source": [
                "---\n",
                "## 13. üìö Additional Resources\n",
                "\n",
                "### Papers:\n",
                "- [Original GAN Paper](https://arxiv.org/abs/1406.2661) - Goodfellow et al., 2014\n",
                "- [DCGAN Paper](https://arxiv.org/abs/1511.06434) - Radford et al., 2015\n",
                "- [StyleGAN](https://arxiv.org/abs/1812.04948) - Karras et al., 2018\n",
                "\n",
                "### Interactive Tools:\n",
                "- [GAN Lab](https://poloclub.github.io/ganlab/) - Visualize GAN training in browser\n",
                "- [ThisPersonDoesNotExist.com](https://thispersondoesnotexist.com) - StyleGAN faces\n",
                "\n",
                "### Tutorials:\n",
                "- [TensorFlow DCGAN Tutorial](https://www.tensorflow.org/tutorials/generative/dcgan)\n",
                "- [PyTorch GAN Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
                "\n",
                "---\n",
                "\n",
                "**üéâ Congratulations! You've built and trained a GAN!**\n",
                "\n",
                "*Next: Transformers & Attention Mechanisms*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}